# =============================================================================
# Data Loading and Preprocessing
# =============================================================================
setwd("./")
data = read.table("MS.csv",sep=",",header=T,row.names=1)
#### Scale all features
data[,1:99] = scale(data[,1:99])
# =============================================================================
# Data Partitioning
# =============================================================================
#### Split data into training (70%) and validation (30%) sets
train_index <- createDataPartition(data$diagnosis, p = 0.7, list = FALSE, 
                                   groups = min(length(unique(data$diagnosis)), 5))
train <- data[train_index, ]
test <- data[-train_index, ]
# =============================================================================
# Feature Selection with Boruta Algorithm
# =============================================================================
library("Boruta")
library(pacman)
p_load("kableExtra","caret","tidyverse","reshape2","e1071","JamesTools","OptimalCutpoints","Boruta","ggplot2","randomForest","ROCR","rpart","party","rpart.plot","partykit","glmnet","xgboost","Ringo", "parallel", "doParallel", "DMwR", "cowplot", "pROC", "readxl")
train$diagnosis <- as.factor(train$diagnosis)
multi.boruta<- list()
# Define function for parallel Boruta execution
multi<- function(i){
  fit.boruta <- Boruta(factor(diagnosis)~., data=train, maxRuns = 300, pValue = 0.01)
  boruta.df <- data.frame(attStats(fit.boruta))
  multi.boruta[[i]] <- rownames(boruta.df[boruta.df$decision =='Confirmed',])
}
# Execute Boruta feature selection with 100 iterations
set.seed(100)
multi.boruta<- parallel::mclapply(1:100, multi)
multi.boruta<- as.data.frame(table(unlist(multi.boruta)))  #### Count frequency of each feature across 100 runs
multi.boruta.m<- as.character(multi.boruta[which(multi.boruta$Freq>10),1])  #### Set frequency threshold for feature selection
multi.boruta
train$diagnosis <- as.factor(train$diagnosis)
# Prepare formula and datasets with selected features
m<- paste(as.vector(multi.boruta.m, mode = "any"), collapse = "+")
RFm<- as.formula(paste("diagnosis ~ ",m,sep = ""))
Boruta.data<- train[,c("diagnosis",multi.boruta.m)]
colnames(test)[37] = "diagnosis"
tree.Boruta<- test[,c("diagnosis", multi.boruta.mrs)]

# =============================================================================
# Random Forest Model Training with Hyperparameter Tuning
# =============================================================================
# Configure repeated cross-validation settings
control<- trainControl(method = 'repeatedcv',
                       number = 10,
                       repeats = 10, classProbs = TRUE, savePredictions = TRUE)
metric <- "Accuracy"
# Define tuning grid for mtry parameter
tunegrid <- expand.grid(.mtry=seq(from =1, to =4))  #### mtry- number of variables sampled at each split
modellist<- list() #### ntree - number of trees to grow
for (ntree in c(100, 250, 500, 750, 1000, 1250, 1500, 1750, 2000)) {
  set.seed(100)
  fit <- caret::train(RFm, data=Boruta.data, method="rf", metric=metric, tuneGrid=tunegrid, trControl=control, ntree=ntree)
  key <- toString(ntree)
  modellist[[key]] <- fit
}
results <- resamples(modellist)
res<- summary(results)
res<- as.data.frame(res$statistics$Accuracy)
summary(results)
ntree = as.numeric(rownames(res)[which(res$Mean == max(res$Mean))])[1] #### Optimize mtry
# Retrain model with optimized ntree parameter
set.seed(100)
tunegrid <- expand.grid(.mtry=seq(from = 1, to=4, by = 1))
modellist<- list()
control<- trainControl(method = 'repeatedcv',
                       number = 10,
                       repeats = 10, classProbs = TRUE, savePredictions = TRUE)
set.seed(100)
fit <- caret::train(RFm, data=Boruta.data, method="rf", metric="Accuracy", tuneGrid=tunegrid, trControl=control, ntree=ntree)
fit
# Train final random forest model with optimized parameters
tunegrid <- expand.grid(.mtry=fit$bestTune$mtry)
control<- trainControl(method = 'repeatedcv',
                       number = 10,
                       repeats = 10,
                       savePredictions = TRUE,
                       classProbs = TRUE)
fit.Boruta <- caret::train(RFm, data=Boruta.data, method="rf", metric="Accuracy", tuneGrid=tunegrid, trControl=control, ntree=ntree)
# Generate predictions on training set
RFBoruta.train <- predict(fit.Boruta, tree.Boruta)
# Calculate performance metrics on training set
BorutaInterim<- confusionMatrix(as.factor(RFBoruta.train), as.factor(tree.Boruta$diagnosis), positive = "RxTB") #### Plot confusion matrix on training data
BorutaInterim
pre = RFBoruta.train
# Predict on validation set using the final random forest model
RFBoruta.train <- predict(fit.Boruta, tree.Boruta,type="prob")
df1 = data.frame(RFBoruta.train)
df1$pre = pre
df1$obs = test$diagnosis
write.csv(df1,"RF-test-pre.csv")
DF = read.table("./train-RF-pre.csv",sep=",",header=T)
confusionMatrix(as.factor(DF$pred), as.factor(DF$obs), positive = "RxTB") #### Plot confusion matrix on validation data

# =============================================================================
# Rpart Decision Tree Analysis with Cross-Validation
# =============================================================================
# Configure repeated cross-validation settings
tc <- trainControl(method="repeatedcv", number=10, repeats = 10, classProbs=TRUE, summaryFunction = twoClassSummary, savePredictions = TRUE)
set.seed(20)
# Train decision tree model with specified parameters
fit.caret.rpart <- caret::train(diagnosis ~ ., data=train, method='rpart', metric="ROC", trControl=tc, control=rpart.control(minsplit=2, minbucket=3, surrogatestyle = 1, maxcompete = 0)) 
# Extract optimal tuning parameters
fit.caret.rpart$bestTune
# Plot party-style decision tree
prp(fit.caret.rpart$finalModel, main="RxTB from TB Rpart model", extra=2, varlen=0)
plot(as.party(fit.caret.rpart$finalModel), main="RxTB from TB Rpart model", drop_terminal=F)
# Extract and clean important features from the model
rpartm<-labels(fit.caret.rpart$finalModel)[-1]
rpartm<- gsub("<.*","", rpartm)
rpartm<- unique(gsub(">.*","", rpartm))
# Model Prediction and Performance Assessment
predrpart2 <- predict(fit.caret.rpart, test)
# Create prediction results table
fit.preds.table.Rpart<- cbind(rownames(test),predrpart2,as.character(test$diagnosis))
fit.preds.table.Rpart<- as.data.frame(fit.preds.table.Rpart)
colnames(fit.preds.table.Rpart) <- c("sample","fit.preds","diagnosis")
#Calculate model performance metrics
test$diagnosis=as.factor(test$diagnosis)
RpartInterim<- confusionMatrix(predrpart2, test$diagnosis, positive = "RxTB")
RpartInterim

# Generate class probability predictions
predrpart2 <- predict(fit.caret.rpart, test)
pre = predrpart2
predrpart2 <- predict(fit.caret.rpart, test,type="prob")
predrpart2 = data.frame(predrpart2)
predrpart2$pre=pre
predrpart2$obs=test$diagnosis
write.csv(predrpart2,"rpart-test-pre.csv")
write.csv(DF,"rpart-train-pre.csv")
DF = read.table("rpart-train-pre.csv",sep=",",header=T,row.names=1)
DF$obs=as.factor(DF$obs)
DF$pred=as.factor(DF$pred)
confusionMatrix(DF$pred,DF$obs, positive = "RxTB")

# Generate Precision-Recall curve
library(PRROC)
df1 = read.table("Rpart-test-pre-SMOTE.csv",sep=",",header=T,row.names=1)
pr = pr.curve(scores.class0 = df1[,1], weights.class0 = df1[,5],curve = T)
plot(pr, main="PR Curve lasso", col="#1c61b6", lwd=2)

# =============================================================================
# LASSO Regression Analysis with Cross-Validation
# =============================================================================
# Perform 10-fold cross-validation for LASSO regression
fit.glmcv <- cv.glmnet(x=as.matrix(train[-37]), y=as.factor(train$diagnosis), alpha=1, family='binomial', nfolds=10)
#summary(fit.glmcv)
# Alternative CV using misclassification error as metric
other.glmcv <- cv.glmnet(x=as.matrix(train[-37]), y=as.factor(train$diagnosis), alpha=1, family='binomial', nfolds=10, type.measure = "class")
# Extract optimal lambda value
model.lambda <- fit.glmcv$lambda.min
# Plot cross-validation results
plot(fit.glmcv, cex.axis=1, cex.lab=1,cex.main=1)
# Display optimal lambda value
paste("lambda value:", fit.glmcv$lambda.min)
# Define tuning grid with optimal lambda
tuneLASSO<- expand.grid(.alpha = 1, .lambda = fit.glmcv$lambda.min)
# Train final LASSO model using caret
LASSO.min<- caret::train(diagnosis ~ ., data = train, method = "glmnet", trControl = control, family = 'binomial', tuneGrid = tuneLASSO)
# Extract non-zero coefficients from final model
lasso.model<- coef(LASSO.min$finalModel, LASSO.min$bestTune$lambda) %>% as.matrix() %>% as.data.frame() %>% rownames_to_column() %>% setNames(c("rowname", "coefficient")) %>% filter(abs(`coefficient`) >0)
# Create dataset with LASSO-selected features
ml.Spear.LASSO <- cbind("group" =train$diagnosis, train[,colnames(train) %in% lasso.model$rowname])
# Report number of selected features
paste("Number of features in LASSO model:",length(lasso.model$rowname)-1)
# Display selected features and coefficients
kable(lasso.model, caption = "features retained by LASSO") %>% kable_styling(full_width = TRUE)
# Prepare test set with matching features
val.LASSO<- test[, colnames(test) %in% colnames(train)]
# Generate class predictions
LASSO.pred.min <- predict(LASSO.min, newdata=val.LASSO)
predicted.classes<-as.character(LASSO.pred.min)
# Create prediction results table
fit.preds.table.LASSO<- as.data.frame(cbind(as.character(rownames(test)),as.character(predicted.classes),as.character(test$diagnosis)))
colnames(fit.preds.table.LASSO) <- c("sample","LASSO","diagnosis")
# Calculate confusion matrix and performance metrics
val.LASSO$diagnosis=as.factor(val.LASSO$diagnosis)
LASSOInterim<- confusionMatrix(LASSO.pred.min,val.LASSO$diagnosis, positive = "RxTB")
LASSOInterim
# Generate class probability predictions
LASSO.pred.min <- predict(LASSO.min, newdata=val.LASSO)
pre = LASSO.pred.min
LASSO.pred.min <- predict(LASSO.min, newdata=val.LASSO,type = "prob")
LASSO.pred.min=data.frame(LASSO.pred.min)
LASSO.pred.min$pre=pre
LASSO.pred.min$obs = val.LASSO$diagnosis
write.csv(LASSO.pred.min,"lasso-test-pre.csv")
write.csv(DF,"lasso-train-pre.csv")
# Calculate performance metrics on validation set
confusionMatrix(DF$pred,DF$obs, positive = "RxTB")



# =============================================================================
# XGBoost Model Setup and Data Preparation
# =============================================================================
library(xgboost)
# Prepare feature matrices by removing outcome column
train1<- train[-37]
train1<- data.matrix(train1)
validation<- test[-37]
validation<- data.matrix(validation)
# Extract outcome labels
labels<- train$diagnosis

# =============================================================================
# Baseline XGBoost Model with Default Parameters
# =============================================================================
# Define default hyperparameter grid
grid_default <- expand.grid(
  nrounds = 100,
  max_depth = 6,
  eta = 0.3,
  gamma = 0,
  colsample_bytree = 1,
  min_child_weight = 1,
  subsample = 1
)
# Configure training control without cross-validation
train_control <- caret::trainControl(
  method = "none",
  verboseIter = FALSE, # no training log
  allowParallel = TRUE 
)
# Train baseline XGBoost model
xgb_base<- caret::train( 
  x = train1,
  y = as.factor(labels),
  trControl = train_control,
  tuneGrid = grid_default,
  method = "xgbTree",
  verbose = TRUE,
  scale_pos_weight = (178/89) #(Total no. samples / no. positive in groupA)
)
# Evaluate baseline model performance
xgbpredict_base<- predict(xgb_base, validation)
confusionMatrix(xgbpredict_base, as.factor(test$diagnosis), positive = "RxTB")
# =============================================================================
# Hyperparameter Tuning - Round 1: nrounds and max_depth
# =============================================================================
# Define nrounds sequence
nrounds<- seq(from = 100, to =1000, by = 50)
# Create first tuning grid
tune_grid <- expand.grid(
  #nrounds = seq(from = 100, to = 1000, by = 20),
  nrounds = nrounds,
  eta = 0.05,
  max_depth = c(1, 2, 3),
  gamma = 0,
  colsample_bytree = 1,
  min_child_weight = 1,
  subsample = 1
)
# Configure repeated cross-validation
set.seed(25)
tune_control <- caret::trainControl(
  method = "repeatedcv", # cross-validation
  number = 10, # with n folds
  repeats = 10, #the no. of complete sets of folds to compute
  #index = createFolds(tr_treated$Id_clean), # fix the folds
  verboseIter = FALSE, # no training log
  allowParallel = TRUE 
)
# Perform first round of hyperparameter tuning
set.seed(25)
xgb_tune <- caret::train(
  x = train1,
  y = as.factor(labels),
  trControl = tune_control,
  tuneGrid = tune_grid,
  method = "xgbTree",
  verbose = FALSE
)
# Display best parameters from first tuning round
kable(xgb_tune$bestTune, caption = "1st tuning, best parameters")%>% kable_styling(full_width = TRUE)
# =============================================================================
# Hyperparameter Tuning - Round 2: min_child_weight
# =============================================================================
# Refine tuning grid based on previous results
tune_grid2 <- expand.grid(
  nrounds = nrounds,
  eta = 0.05,
  max_depth = c(xgb_tune$bestTune$max_depth -1, xgb_tune$bestTune$max_depth, xgb_tune$bestTune$max_depth +1),
  gamma = 0,
  colsample_bytree = 1,
  min_child_weight = c(0,1,2,3,4),
  subsample = 1
)
# Perform second round of hyperparameter tuning
set.seed(25)
xgb_tune2 <- caret::train(
  x = train1,
  y = as.factor(labels),
  trControl = tune_control,
  tuneGrid = tune_grid2,
  method = "xgbTree",
  verbose = TRUE
)
# Display best parameters from second tuning round
kable(xgb_tune2$bestTune, caption = "2nd tuning, best parameters")%>% kable_styling(full_width = TRUE)
# =============================================================================
# Hyperparameter Tuning - Round 3: colsample_bytree and subsample
# =============================================================================
# Further refine subsampling parameters
tune_grid3 <- expand.grid(
  nrounds = nrounds,
  eta = 0.05,
  max_depth = xgb_tune2$bestTune$max_depth,
  gamma = 0,
  colsample_bytree = seq(from = 0, to = 1, by = 0.1),
  min_child_weight = xgb_tune2$bestTune$min_child_weight,
  subsample = seq(from = 0, to = 1, by = 0.1)
)
# Perform third round of hyperparameter tuning
set.seed(25)
xgb_tune3 <- caret::train(
  x = train1,
  y = as.factor(labels),
  trControl = tune_control,
  tuneGrid = tune_grid3,
  method = "xgbTree",
  verbose = TRUE
  #,   scale_pos_weight = (35/21) -1
)
# Display best parameters from third tuning round
kable(xgb_tune3$bestTune, caption = "3rd tuning, best parameters")%>% kable_styling(full_width = TRUE)
# =============================================================================
# Hyperparameter Tuning - Round 4: gamma (regularization)
# =============================================================================
# Tune gamma parameter for regularization
tune_grid4 <- expand.grid(
  nrounds = nrounds,
  eta = 0.05,
  max_depth = xgb_tune3$bestTune$max_depth,
  gamma = seq(from = 0, to = 1, by = 0.05),
  colsample_bytree = xgb_tune3$bestTune$colsample_bytree,
  min_child_weight = xgb_tune2$bestTune$min_child_weight,
  subsample = xgb_tune3$bestTune$subsample
)
# Perform fourth round of hyperparameter tuning
set.seed(25)
xgb_tune4 <- caret::train(
  x = train1,
  y = as.factor(labels),
  trControl = tune_control,
  tuneGrid = tune_grid4,
  method = "xgbTree",
  verbose = TRUE
)
# Display best parameters from fourth tuning round
kable(xgb_tune4$bestTune, caption = "4th tuning, best parameters")%>% kable_styling(full_width = TRUE)
# =============================================================================
# Hyperparameter Tuning - Round 5: nrounds and eta (final refinement)
# =============================================================================
# Final tuning with expanded search ranges
tune_grid5 <- expand.grid(
  nrounds = seq(from = 100, to = 10000, by = 50),
  eta = seq(from = 0, to = 1, by = 0.05),
  max_depth = xgb_tune3$bestTune$max_depth,
  gamma = xgb_tune4$bestTune$gamma,
  colsample_bytree = xgb_tune3$bestTune$colsample_bytree,
  min_child_weight = xgb_tune2$bestTune$min_child_weight,
  subsample = xgb_tune3$bestTune$subsample
)
# Perform final round of hyperparameter tuning
set.seed(25)
xgb_tune5 <- caret::train(
  x = train1,
  y = as.factor(labels),
  trControl = tune_control,
  tuneGrid = tune_grid5,
  method = "xgbTree",
  verbose = TRUE
)
# Display final optimized parameters
kable(xgb_tune5$bestTune, caption = "5th tuning, best parameters")%>% kable_styling(full_width = TRUE)
# =============================================================================
# Final Model Training and Feature Importance
# =============================================================================
# Convert labels to binary format for XGBoost
xgblabels <- ifelse(labels == "RxTB", 1,0)
# Train final XGBoost model with optimized parameters
xgb_final<- xgboost(data = train1, label = xgblabels, nrounds = xgb_tune5$bestTune$nrounds, objective = "binary:logistic", max_depth = xgb_tune5$bestTune$max_depth, eta = xgb_tune5$bestTune$eta, min_child_weight = xgb_tune5$bestTune$min_child_weight, verbose = 0)
# Calculate and display feature importance
importance_final<- xgb.importance(feature_names = colnames(train1), model = xgb_final)
xgb_ms<- importance_final[importance_final$Gain>0.05,]
kable(xgb_ms, caption = "Important Features (Gain > 0.05)")%>% kable_styling(full_width = TRUE)
# Visualize feature importance
gg<- xgb.ggplot.importance(importance_matrix = xgb_ms)
gg+ggplot2::theme(legend.position = "none", text = element_text(size = 22), axis.text = element_text(size = 18))
# =============================================================================
# Model Evaluation on Test Set
# =============================================================================
# Generate final predictions
xgbpredictfinal<- predict(xgb_tune5, validation)
xgbpredictfinal_probs<- predict(xgb_tune5, validation, type = "prob")
# Calculate final performance metrics
confusionMatrix(xgbpredictfinal, as.factor(test$diagnosis), positive = "RxTB")
# Create datasets with important features only
setAshort<- train[,c("diagnosis",xgb_ms$Feature)]
setBshort<- test[,c("diagnosis",xgb_ms$Feature)]
train.short<- data.matrix(setAshort[-1])
validation.short<- data.matrix(setBshort[-1])
labels<- setAshort$diagnosis 
setBshort$diagnosis <- test$diagnosis
# Generate predictions from all models
Boruta.perf<- predict(fit.Boruta, tree.Boruta[-1], type= "prob")
rpart.perf<- predict(fit.caret.rpart, test[-37], type= "prob")
LASSO.probs.min <- predict(LASSO.min, newdata=val.LASSO[-37], type = "prob")
xgbpreds<- predict(xgb_tune_final_short, validation.short, type = "prob")
# Combine predictions for ensemble analysis
totalprobs<- data.frame(patientID = rownames(Boruta.perf), group = test$diagnosis, RandomForest = Boruta.perf$RxTB, rpart = rpart.perf$RxTB, LASSO = LASSO.probs.min$RxTB, XGB = xgbpreds$RxTB)
# Calculate ensemble predictions
totalprobs <- mutate(totalprobs, Mean = rowMeans(totalprobs[3:6]))
totalprobs$EnsemblePreds <- ifelse(totalprobs$Mean > 0.5, "RxTB", "TB")
# Generate ensemble performance metrics
Ensemble.pred<-prediction(totalprobs$Mean, totalprobs$group) 
Ensemble.perfs<- ROCR::performance(Ensemble.pred, "tpr", "fpr")
# Display ensemble results
kable(totalprobs)  %>% kable_styling(full_width = TRUE)
table(totalprobs$EnsemblePreds, totalprobs$group)
# =============================================================================
# Biomarker validation using public dataset
# =============================================================================
# Set working directory and load public dataset
setwd("./NCBI/")
data=read.table("TB treatment monitor1.csv",sep=",",row.names=1,header=T)
group=read_xlsx("group_GSE.xlsx",sheet=1)
# Filter samples for TB and W26 timepoints
group1 <- group[group$group %in% c("TB", "W26"), ]
data1=data[,colnames(data) %in% group1$sample]
# Normalize data and visualize distribution
exprSet=normalizeBetweenArrays(exprSet)
boxplot(data1)
# =============================================================================
# Differential Expression Analysis
# =============================================================================
# Create experimental design matrix
group2 <- factor(c(rep("control",8), rep("RIF", 10)), levels = c('control', 'RIF'))
design <- model.matrix(~0+group2)
# Prepare group information for linear modeling
group1$W26=group1$group
group1=data.frame(group1)
rownames(group1)=group1$sample
group1=group1[,-1]
colnames(group1)[1]="TB"
group1[, 2] <- ifelse(group1[, 2] == "TB", 0, ifelse(group1[, 2] == "W26", 1, NA))
design=group1
exprSet=data1
# Perform differential expression analysis
compare <- makeContrasts(W26 - TB, levels=design)
fit <- lmFit(exprSet, design)
fit <- contrasts.fit(fit, compare)
fit <- eBayes(fit)
Diff <- topTable(fit, coef=1, number=Inf)
# Count significant differentially expressed genes
table(Diff$logFC>1,Diff$P.Value<0.05)
table(Diff$logFC< -1,Diff$P.Value<0.05)
# Export results
write.csv(exprSet,"GSE92324.csv")
write.csv(Diff,"GSE92324.csv")
# =============================================================================
# GSVA Pathway Analysis
# =============================================================================
library(GSVA)
library(GSEABase)
# Load expression data and gene sets
data=read.xlsx("GSE87017_2016_RoadmapProcessedDataFile.xlsx",sheet=7)
geneSets <- getGmt("pathway.gmt")
# Prepare expression matrix
rownames(data)=data[,2]
data=data[,-(1:2)]
data <- as.matrix(data)
# Convert character data to numeric (critical step)
data[,1:ncol(data)]<-as.numeric(unlist(data[,1:ncol(data)]))
# Perform GSVA analysis
mygeneset = as.data.frame(geneSets)
es_es <- gsva(data, geneSets, min.sz=5, max.sz=1000, verbose=FALSE, parallel.sz=1)
es = gsva(data, geneSets,min.sz=5, max.sz=1000,mx.diff=FALSE, verbose=FALSE, parallel.sz=1,method='gsva',kcdf="Gaussian")
es_es=data.frame(es)

# =============================================================================
# Feature Selection Consensus Analysis
# =============================================================================
# Load feature selection results from different methods
set.seed(123)
N_features <- 100 
methods <- c("LASSO", "RF", "SVM", "XGB")
df=read.xlsx("./feature selection.xlsx",sheet=1)
# Extract features selected by each method
Boruta_features <- df[1:28,2]
Rpart_features <- df[1:4,3]
XGBoost_features <- df[1:22,4]
LASSO_features <- df[1:5,5]
# Extract features selected by each method
all_unique_features <- unique(c(Boruta_features, Rpart_features, XGBoost_features, LASSO_features))
binary_matrix <- data.frame(
  Feature = all_unique_features,
  LASSO = as.integer(all_unique_features %in% LASSO_features),
  RandomForest = as.integer(all_unique_features %in% Boruta_features),
  SVM = as.integer(all_unique_features %in% Rpart_features),
  XGBoost = as.integer(all_unique_features %in% XGBoost_features)
)
head(binary_matrix)
rating_matrix <- as.matrix(binary_matrix[, -1])
rownames(rating_matrix) <- binary_matrix$Feature
# =============================================================================
# Inter-Method Agreement Assessment
# =============================================================================
# Calculate Fleiss' Kappa for multiple raters
kappa_result <- kappam.fleiss(rating_matrix, detail = TRUE)
print(kappa_result)
# =============================================================================
# Jaccard Similarity Analysis
# =============================================================================
# Define Jaccard similarity function
common_features <- sample(which(rowSums(feature_selection[, -1]) == 0), 3)
feature_selection[common_features, -1] <- 1  
# Define Jaccard similarity function
jaccard_matrix <- function(df) {
  methods <- colnames(df)
  n <- length(methods)
  jaccard <- matrix(NA, n, n, dimnames = list(methods, methods))
  
  for (i in 1:n) {
    for (j in 1:n) {
      a <- df[, i]
      b <- df[, j]
      intersection <- sum(a & b)
      union <- sum(a | b)
      jaccard[i, j] <- ifelse(union == 0, 0, intersection / union)
    }
  }
  return(jaccard)
}
# Calculate Jaccard similarity matrix
jaccard <- jaccard_matrix(feature_selection[, -1])  # 排除feature_id列
print(jaccard)
# Count how many methods selected each feature
consensus_count <- rowSums(feature_selection[, -1])
table(consensus_count) 
# Extract features selected by all methods (strong consensus)
strong_consensus <- feature_selection$feature_id[consensus_count == 4]
print(strong_consensus)
# =============================================================================
# Pairwise Method Comparison
# =============================================================================
library(irr)
# Calculate pairwise Cohen's Kappa
kappa_results <- list()
pairs <- combn(methods, 2, simplify = FALSE)
head(rating_matrix)
for (pair in pairs) {
  method1 <- pair[1]
  method2 <- pair[2]
  Cohen. kappa(rating_matrix[,1],rating_matrix[,2])
  kappa_results[[paste(method1, "vs", method2)]] <- kappa
}
print(kappa_results)
library(irr)
fleiss_data <- as.matrix(feature_selection[, -1])
fleiss_kappa <- kappam.fleiss(rating_matrix, exact = TRUE, detail = TRUE)
print(fleiss_kappa)
# =============================================================================
# Intraclass Correlation Coefficient (ICC) Analysis
# =============================================================================
library(psych)
# Generate example importance scores (replace with actual data)
importance_scores <- data.frame(
  LASSO = rnorm(N_features),
  RF    = rnorm(N_features),
  SVM   = rnorm(N_features),
  XGB   = rnorm(N_features)
)

# Calculate ICC for method agreement
icc_result <- ICC(importance_scores, lmer = FALSE)
print(icc_result)
# =============================================================================
# Visualization of Method Similarity
# =============================================================================
library(pheatmap)
# Create heatmap of Jaccard similarities
pheatmap(jaccard, 
         cluster_rows = FALSE, 
         cluster_cols = FALSE,
         display_numbers = TRUE,
         main = "Jaccard Similarity Between Methods")
methods <- colnames(rating_matrix)
pairs <- combn(methods, 2, simplify = FALSE)
# Generate all pairwise combinations of feature selection methods
methods <- colnames(rating_matrix)
pairs <- combn(methods, 2, simplify = FALSE)

# Define custom Jaccard similarity function
jaccard_custom <- function(x, y) {
  intersection <- sum(x == 1 & y == 1)
  union <- sum(x == 1 | y == 1)
  return(ifelse(union == 0, 0, intersection / union))
}
jaccard_custom_results <- sapply(pairs, function(pair) {
  jaccard_custom(rating_matrix[,pair[1]], rating_matrix[,pair[2]])
})
result_df <- data.frame(
  Pair = sapply(pairs, paste, collapse = " vs "),
  Custom_Jaccard = jaccard_custom_results
)
jaccard_matrix <- outer(1:ncol(rating_matrix), 1:ncol(rating_matrix), 
                        Vectorize(function(i, j) 
                          jaccard_custom(rating_matrix[,i], rating_matrix[,j])))
colnames(jaccard_matrix) <- methods
rownames(jaccard_matrix) <- methods
library(ggplot2)
library(reshape2)
ggplot(melt(jaccard_matrix), aes(Var1, Var2, fill = value)) + 
  geom_tile() + 
  scale_fill_gradient(low = "white", high = "darkblue") +
  labs(title = "Jaccard similarity")

